---
author: Sebastian Barfort
title: "Social Data Science"
subtitle: Causation & Prediction 
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  beamer_presentation:
    keep_tex: no
    theme: metropolis
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    fig_width: 7
    fig_height: 6
    fig_caption: false
    includes:
      in_header: header.tex
fontsize: 10pt
classoption: compress
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
knitr::opts_chunk$set(
              dev= "pdf",
               fig.width=4.25,
               fig.height=2.5,
               fig.show="hold",
               fig.lp="fig:",
               fig.align = "center",
               dpi = 300,
               cache=TRUE,
               par=TRUE,
               echo=TRUE,
               message=FALSE,
               warning=FALSE)
```

## Introduction

What is the objective of empirical policy research? 

1. \alert{causation}: what is the effect of a particular variable on an outcome? 
2. \alert{prediction}: given what we know, what is our best prediction of new outcomes? 

Today: Short introduction 

## Intution

$$ y = \alpha + \beta x + \varepsilon $$

\alert{causation}: $\hat{\beta}$ problem

\alert{prediction}: $\hat{y}$ problem

---

\LARGE Causal Inference

## Introduction

Most econometric theory is focused on estimating **causal effects**

Causal effect: what is the effect of some policy on an outcome we are interested in?

Examples of causal questions:

- what is the effect of immigration on native wages?
- what is the effect of democracy on growth? 
- what is the effect of newspaper coverage on stock prices?

## Intuition

Variable of interest (often called *treatment*): $D_i$

Outcome of interest: $Y_i$

**Potential outcome framework**
$$
Y_i = \left\{
\begin{array}{rl}
Y_{1i} & \text{if } D_i = 1,\\
Y_{0i} & \text{if } D_i = 0
\end{array} \right.
$$

The observed outcome $Y_i$ can be written in terms of potential outcomes as
$$ Y_i = Y_{0i} + (Y_{1i}-Y_{0i})D_i$$

$Y_{1i}-Y_{0i}$ is the *causal* effect of $D_i$ on $Y_i$. 

But we never observe the same individual $i$ in both states (treated & non-treated). 

This is the **fundamental problem of causal inference**. 

## Selection bias

We need some way of estimating the state we do not observe (the *counterfactual*)

Usually, our sample contains individuals from both states

So why not do a naive comparison of averages by treatment status? 
\begin{align}
\nonumber E[Y_i|D_i = 1] - E[Y_i|D_i = 0] = &E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] + \\
 \nonumber  &E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0] 
\end{align}

$E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] = E[Y_{1i} - Y_{0i}|D_i = 1]$: the average *causal* effect of $D_i$ on $Y$. 

$E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]$: difference in average $Y_{0i}$ between the two groups. Likely to be different from 0 when individuals are allowed to self-select into treatment. Often referred to as \alert{selection bias} or \alert{confounding}. 

## Random assignment solves the problem

Random assignment of $D_i$ solves the problem because random assignment makes $D_i$ independent of potential outcomes

That means that $E[Y_{0i}|D_i = 1] = E[Y_{0i}|D_i = 0]$ and thus that the selection bias term is zero

Intuition: with random assignment, non-treated individuals can be used as counterfactuals for treated (*what would have happened to individual $i$ had he not received the treatment*?)

This allows us to overcome the fundamental problem of causal inference

## Who randomizes? 

> “no causation without manipulation”

*Paul Holland (1986)*

As mentioned, we need to worry when individuals are allowed to self-select

This means that a lot of thought has to go into the *randomization phase*

Randomization into treatment groups has to be manipulated by someone 

But what about effect of \alert{immutable characteristics} such as race, gender, etc.?

## Who manipulates?

\alert{Quasi-experiments}: randomization happens by "accident"

- Differences in Differences
- Regression Discontinuity Design
- Instrumental variables

\alert{Randomized controlled trials}: randomization done by researcher

- Survey experiments
- Field experiments

Note: difficult to say one is strictly better than the other. Randomization can be impractical and/or unethical. 

Can you come up with an example where randomization would be unethical?

## External & internal validity

\alert{Internal validity}: Refers to the validity of causal conclusions

\alert{External validity}: Refers to the extent to which the conclusions of a particular study can be generalized beyond a particular setting

Imai (2016): RCTs trade off external and internal validity

Samii (2016): No such tradeoff. 

## Observational study

In many cases, social scientists are unable to randomize treatment assignment for ethical or logistic reasons

\alert{Observational study}: No random manipulation of treatment

Strategy: Statistical control (fixed effects, matching, etc)

Risk selection/confounding bias. 

## Pritchett & Sandefur

Pritchett, Lant and Justin Sandefur. 2015. "[Learning from Experiments When Context Matters](https://www.aeaweb.org/articles?id=10.1257/aer.p20151016)." *American Economic Review*, 105(5): 471-75. 

> We analyze the trade-off between internal and external validity faced by a hypothetical policymaker weighing experimental and non- experimental evidence. Empirically, we find that for several prominent questions in develop- ment economics, relying on observational data analysis from within context produces treat- ment effect estimates with lower mean-square error than relying on experimental estimates from another context.


## Racial Discrimination in the Labor Market

Does racial discrimination exist in the labor market?

Experiment: In response to newspaper ads, researchers send out resumes of fictitious job candidates, varying only the names of the job applicants while leaving all other information in the resumes unchanges

Names were randomized between stereotypically black- and white-sounding names (Lakisha, Jamal, Emily, Greg, etc.)

## 

```{r}
library("readr")
gh.link = "https://raw.githubusercontent.com/"
user.repo = "kosukeimai/qss/"
branch = "master/"
link = "CAUSALITY/resume.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## 

```{r, echo = FALSE}
knitr::kable(df[1:5, ])
```

## Contingency table

```{r}
library("dplyr")
df.table = df %>% 
  count(race, call)
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Proportions

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## By gender

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, sex, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

##

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Systematic differences?

Difference in means estimator

```{r}
lin.model = lm(call ~ race == "black", 
               data = df)
```

##

```{r, results = "asis", echo = FALSE}
stargazer::stargazer(lin.model, header = FALSE,
                     no.space = TRUE)
```

## Summary

Causal questions are of key interest to policy makers and academics

The key focus is on *inference*: we want to know about the causal effect of $D$ on $Y$ *in the population of interest*

When you are interested in a **causal question** you need to think carefully about randomization of treatment (this is often referred to as your *identification strategy*)

Is causality the only thing policy makers and social scientists should be interested in?

