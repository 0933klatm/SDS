---
author: Sebastian Barfort
title: "Social Data Science"
subtitle: Causation & Prediction 
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  beamer_presentation:
    keep_tex: no
    theme: metropolis
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    fig_width: 7
    fig_height: 6
    fig_caption: false
    includes:
      in_header: header.tex
fontsize: 10pt
classoption: compress
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
knitr::opts_chunk$set(
              dev= "pdf",
               fig.width=4.25,
               fig.height=2.5,
               fig.show="hold",
               fig.lp="fig:",
               fig.align = "center",
               dpi = 300,
               cache=TRUE,
               par=TRUE,
               echo=TRUE,
               message=FALSE,
               warning=FALSE)
```

## Introduction

What is the objective of empirical policy research? 

1. \alert{causation}: what is the effect of a particular variable on an outcome? 
2. \alert{prediction}: given what we know, what is our best prediction of new outcomes? 

Today: Short introduction 

## Intution

$$ y = \alpha + \beta x + \varepsilon $$

\alert{causation}: $\hat{\beta}$ problem

\alert{prediction}: $\hat{y}$ problem

---

\LARGE Causal Inference

## Introduction

Most econometric theory is focused on estimating **causal effects**

Causal effect: what is the effect of some policy on an outcome we are interested in?

Examples of causal questions:

- what is the effect of immigration on native wages?
- what is the effect of democracy on growth? 
- what is the effect of newspaper coverage on stock prices?

## Intuition

Variable of interest (often called *treatment*): $D_i$

Outcome of interest: $Y_i$

**Potential outcome framework**
$$
Y_i = \left\{
\begin{array}{rl}
Y_{1i} & \text{if } D_i = 1,\\
Y_{0i} & \text{if } D_i = 0
\end{array} \right.
$$

The observed outcome $Y_i$ can be written in terms of potential outcomes as
$$ Y_i = Y_{0i} + (Y_{1i}-Y_{0i})D_i$$

$Y_{1i}-Y_{0i}$ is the *causal* effect of $D_i$ on $Y_i$. 

But we never observe the same individual $i$ in both states (treated & non-treated). 

This is the **fundamental problem of causal inference**. 

## Selection bias

We need some way of estimating the state we do not observe (the *counterfactual*)

Usually, our sample contains individuals from both states

So why not do a naive comparison of averages by treatment status? 
\begin{align}
\nonumber E[Y_i|D_i = 1] - E[Y_i|D_i = 0] = &E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] + \\
 \nonumber  &E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0] 
\end{align}

$E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] = E[Y_{1i} - Y_{0i}|D_i = 1]$: the average *causal* effect of $D_i$ on $Y$. 

$E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]$: difference in average $Y_{0i}$ between the two groups. Likely to be different from 0 when individuals are allowed to self-select into treatment. Often referred to as \alert{selection bias} or \alert{confounding}. 

## Random assignment solves the problem

Random assignment of $D_i$ solves the problem because random assignment makes $D_i$ independent of potential outcomes

That means that $E[Y_{0i}|D_i = 1] = E[Y_{0i}|D_i = 0]$ and thus that the selection bias term is zero

Intuition: with random assignment, non-treated individuals can be used as counterfactuals for treated (*what would have happened to individual $i$ had he not received the treatment*?)

This allows us to overcome the fundamental problem of causal inference

## Who randomizes? 

> “no causation without manipulation”

*Paul Holland (1986)*

As mentioned, we need to worry when individuals are allowed to self-select

This means that a lot of thought has to go into the *randomization phase*

Randomization into treatment groups has to be manipulated by someone 

But what about effect of \alert{immutable characteristics} such as race, gender, etc.?

## Who manipulates?

\alert{Quasi-experiments}: randomization happens by "accident"

- Differences in Differences
- Regression Discontinuity Design
- Instrumental variables

\alert{Randomized controlled trials}: randomization done by researcher

- Survey experiments
- Field experiments

Note: difficult to say one is strictly better than the other. Randomization can be impractical and/or unethical. 

Can you come up with an example where randomization would be unethical?

## External & internal validity

\alert{Internal validity}: Refers to the validity of causal conclusions

\alert{External validity}: Refers to the extent to which the conclusions of a particular study can be generalized beyond a particular setting

Imai (2016): RCTs trade off external and internal validity

Samii (2016): No such tradeoff. 

## Observational study

In many cases, social scientists are unable to randomize treatment assignment for ethical or logistic reasons

\alert{Observational study}: No random manipulation of treatment

Strategy: Statistical control (fixed effects, matching, etc)

Risk selection/confounding bias. 

## Pritchett & Sandefur

Pritchett, Lant and Justin Sandefur. 2015. "[Learning from Experiments When Context Matters](https://www.aeaweb.org/articles?id=10.1257/aer.p20151016)." *American Economic Review*, 105(5): 471-75. 

> We analyze the trade-off between internal and external validity faced by a hypothetical policymaker weighing experimental and non- experimental evidence. Empirically, we find that for several prominent questions in develop- ment economics, relying on observational data analysis from within context produces treat- ment effect estimates with lower mean-square error than relying on experimental estimates from another context.


## Racial Discrimination in the Labor Market

Does racial discrimination exist in the labor market?

Experiment: In response to newspaper ads, researchers send out resumes of fictitious job candidates, varying only the names of the job applicants while leaving all other information in the resumes unchanges

Names were randomized between stereotypically black- and white-sounding names (Lakisha, Jamal, Emily, Greg, etc.)

## 

```{r}
library("readr")
gh.link = "https://raw.githubusercontent.com/"
user.repo = "kosukeimai/qss/"
branch = "master/"
link = "CAUSALITY/resume.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## 

```{r, echo = FALSE}
knitr::kable(df[1:5, ])
```

## Contingency table

```{r}
library("dplyr")
df.table = df %>% 
  count(race, call)
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Proportions

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## By gender

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, sex, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

##

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Systematic differences?

Difference in means estimator

```{r}
lin.model = lm(call ~ race == "black", 
               data = df)
```

##

```{r, results = "asis", echo = FALSE}
stargazer::stargazer(lin.model, header = FALSE,
                     no.space = TRUE)
```

## Summary

Causal questions are of key interest to policy makers and academics

The key focus is on *inference*: we want to know about the causal effect of $D$ on $Y$ *in the population of interest*

When you are interested in a **causal question** you need to think carefully about randomization of treatment (this is often referred to as your *identification strategy*)

Is causality the only thing policy makers and social scientists should be interested in?

---

\LARGE Prediction

## Prediction

> “It's tough to make predictions, especially about the future.” 

*Storm P*

Many policy problems are not about causality but rather about prediction


## Who predicts?

* Local governments -> pension payments/crime/etc
* Google -> whether you will click on an ad
* Netflix -> what movies you will watch
* Insurance companies -> what your risk of death is
* You? -> will *Social Data Science* be a fun/rewarding/interesting course to follow?

## Why predict? Glory!

\centering
![](http://www2.pictures.zimbio.com/gi/Chris+Volinsky+Netflix+Awards+1+Million+Netflix+WOtFSoPeoOal.jpg)

[Netflix Awards $1 Million Prize and Starts a New Contest](http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/?8au&emc=au)

## Why predict? Riches!

\centering
![](http://kaggle2.blob.core.windows.net/competitions/hhp/2496/logos/header.png)

[http://www.heritagehealthprize.com/c/hhp](http://www.heritagehealthprize.com/c/hhp)

## Example: predicting gender from weight/height

Can we predict gender based on information on an individual's weight/height?

```{r, message = FALSE, warning = FALSE}
gh.link = "https://raw.githubusercontent.com/"
user.repo = "johnmyleswhite/ML_for_Hackers/"
branch = "master/02-Exploration/"
link = "data/01_heights_weights_genders.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## 

```{r, echo = FALSE}
knitr::kable(df[1:5, ])
```

## 

```{r, echo = FALSE}
library("ggplot2")
library("viridis")
ggplot(df, aes(x = Height, fill = Gender)) + 
  geom_density(alpha = .5) +
  scale_fill_viridis(discrete = TRUE)
```
 
## 

```{r, echo = FALSE}
ggplot(sample_frac(df, .5), aes(x = Weight, y = Height)) + 
  geom_point(alpha = .25) + 
  geom_smooth()
```

##

```{r, echo = FALSE}
ggplot(sample_frac(df, .5), aes(x = Weight, y = Height, 
               colour = Gender)) + 
  geom_point(alpha = .1) +
  scale_color_viridis(discrete = TRUE)
```

## Logit model

```{r}
df = df %>% mutate(gender = Gender == "Male")
logit.model = glm(gender ~ Height + Weight, 
                  data = df, 
                  family = binomial(link = "logit"))
```

## The binomial model

Logit estimates

$$ P(Y_i = 1 |X_i = x_i) = \frac{1}{1 + e^{-x_i\beta}}$$

This probability is .5 when $x_i\beta = 0$

So we can classify predicted gender based on height and weight

$$
\hat{y} = \left\{
\begin{array}{rl}
1 & \text{if } x_i\beta \geq 0,\\
0 & \text{otherwise}
\end{array} \right.
$$

## Calculate classification line

**Intercept**: $$ W = \frac{-\alpha - \beta_1 H}{\beta_2} $$

**Slope**: $$ -\frac{\beta_1}{\beta_2} $$

So now we have a classifier: For each combination of weight and height we're able to predict gender based on our logistic model

## 

```{r, echo = FALSE}
buildPoly <- function(xr, yr, slope = 1, intercept = 0, above = TRUE){
    #Assumes ggplot default of expand = c(0.05,0)
    xrTru <- xr + 0.05*diff(xr)*c(-1,1)
    yrTru <- yr + 0.05*diff(yr)*c(-1,1)

    #Find where the line crosses the plot edges
    yCross <- (yrTru - intercept) / slope
    xCross <- (slope * xrTru) + intercept

    #Build polygon by cases
    if (above & (slope >= 0)){
        rs <- data.frame(x=-Inf,y=Inf)
        if (xCross[1] < yrTru[1]){
            rs <- rbind(rs,c(-Inf,-Inf),c(yCross[1],-Inf))
        }
        else{
            rs <- rbind(rs,c(-Inf,xCross[1]))
        }
        if (xCross[2] < yrTru[2]){
            rs <- rbind(rs,c(Inf,xCross[2]),c(Inf,Inf))
        }
        else{
            rs <- rbind(rs,c(yCross[2],Inf))
        }
    }
    if (!above & (slope >= 0)){
        rs <- data.frame(x= Inf,y= -Inf)
        if (xCross[1] > yrTru[1]){
            rs <- rbind(rs,c(-Inf,-Inf),c(-Inf,xCross[1]))
        }
        else{
            rs <- rbind(rs,c(yCross[1],-Inf))
        }
        if (xCross[2] > yrTru[2]){
            rs <- rbind(rs,c(yCross[2],Inf),c(Inf,Inf))
        }
        else{
            rs <- rbind(rs,c(Inf,xCross[2]))
        }
    }
    if (above & (slope < 0)){
        rs <- data.frame(x=Inf,y=Inf)
        if (xCross[1] < yrTru[2]){
            rs <- rbind(rs,c(-Inf,Inf),c(-Inf,xCross[1]))
        }
        else{
            rs <- rbind(rs,c(yCross[2],Inf))
        }
        if (xCross[2] < yrTru[1]){
            rs <- rbind(rs,c(yCross[1],-Inf),c(Inf,-Inf))
        }
        else{
            rs <- rbind(rs,c(Inf,xCross[2]))
        }
    }
    if (!above & (slope < 0)){
        rs <- data.frame(x= -Inf,y= -Inf)
        if (xCross[1] > yrTru[2]){
            rs <- rbind(rs,c(-Inf,Inf),c(yCross[2],Inf))
        }
        else{
            rs <- rbind(rs,c(-Inf,xCross[1]))
        }
        if (xCross[2] > yrTru[1]){
            rs <- rbind(rs,c(Inf,xCross[2]),c(Inf,-Inf))
        }
        else{
            rs <- rbind(rs,c(yCross[1],-Inf))
        }
    }

    return(rs)
}

datPoly1 <- buildPoly(range(df$Height), range(df$Weight),
            slope= - coef(logit.model)[2] / coef(logit.model)[3],
            intercept= - coef(logit.model)[1] /coef(logit.model)[2],
            above = FALSE)
datPoly2 <- buildPoly(range(df$Height), range(df$Weight),
            slope= - coef(logit.model)[2] / coef(logit.model)[3],
            intercept= - coef(logit.model)[1] /coef(logit.model)[2],
            above = TRUE)
ggplot(sample_frac(df, .5), aes(x = Height, y = Weight)) + 
  geom_point(alpha = 0.2) +
  geom_abline(
    intercept = - coef(logit.model)[1] /coef(logit.model)[2],
    slope = - coef(logit.model)[2] / coef(logit.model)[3],
    color = "black") +
  geom_polygon(data=datPoly1, aes(x=x,y=y),alpha=0.2,fill="red") +
  geom_polygon(data=datPoly2, aes(x=x,y=y),alpha=0.2,fill="blue")
```

## 

```{r, echo = FALSE}
df$prediction = logit.model$fitted.values
df$prediction.cat = ifelse(df$prediction >= .5, "Male", "Female")
df$classified = ifelse(df$prediction.cat == df$Gender, "correct", "incorrect")

df.1 = df %>% filter(classified == "correct") %>% sample_frac(.2)
df.2 = df %>% filter(classified != "correct") %>% sample_frac(.75)

ggplot() + 
  geom_point(data = df.1,
             aes(x = Height, y = Weight),
             alpha = .05) +
  geom_point(data = df.2,
             aes(x = Height, y = Weight,
             fill = abs(prediction - .5)), 
             shape = 21, color = "black",
             alpha = .5) +
  geom_abline(
    intercept = - coef(logit.model)[1] / coef(logit.model)[2],
    slope = - coef(logit.model)[2] / coef(logit.model)[3],
    color = "black") + 
  scale_fill_viridis()
```

## Misclassification

How many cases were misclassified?

```{r}
df.class = df %>% 
  mutate(
    pred.prob = predict(logit.model),
    pred.cat = ifelse(pred.prob >= .5, 
                      "Male", "Female"),
    classified = ifelse(prediction.cat == Gender, 
                        "correct", "incorrect")
  ) %>% 
  group_by(classified) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

## 

```{r, echo = FALSE}
knitr::kable(df.class)
```

##

```{r}
df.class = df %>% 
  mutate(
    pred.prob = predict(logit.model),
    pred.cat = ifelse(pred.prob >= .5, 
                      "Male", "Female")
  ) %>% 
  group_by(Gender, pred.cat) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

##

```{r, echo = FALSE}
knitr::kable(df.class)
```

## How well did we do?

Overall, we correctly classified 92% of the cases in our dataset

Is this a good prediction model? 

Is looking at the classification rate on all the data the correct strategy?


## Introduction to statistical learning

Standard empirical techniques are not optimized for prediction problems because they focus on inference

Standard result in econometrics: When there is no omitted variable bias ($E(\varepsilon) = 0$) and the model is homoskedastic ($V(\varepsilon_i) = \sigma^2$) then the OLS estimator is BLUE (Best Linear Unbiased Estimator). 

Keywords: *unbiased* ($E(\hat{\beta}) = \beta$) and *best* (smallest variance among the class of all linear unbiased estimators)

But what about **biased** estimators?

## The bias-variance tradeoff

OLS is designed to minimize *in sample error*: the error rate you get on the same data set you used to build your predictor.

$$ \text{arg min}_{\beta} \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 $$

But for prediction we are interested in minimizing *out of sample error*: the error rate you get on a new data set

Too see this, consider a prediction at a new point, $x_0$. Our prediction for $y_0$ is then $\hat{f}(x_0)$ and the mean squared error (MSE ) can be decomposed as 

$$ E[(y_0 - \hat{f}(x_0))^2] = [\text{Bias}(\hat{f}(x_0))]^2 + V(\hat{f}(x_0)) + \sigma^2$$

By ensuring zero bias, OLS picks a corner solution. This is generally not optimal for prediction. 


## Bias and variance

What do we mean by the *variance* and *bias* of an estimator?

$\text{Bias}(\hat{f}(x_0)) = E[\hat{f}(x_0) - f(x_0)]$: Bias refers to the error that is introduced by approximating a real-life problem with a simple model. It won't fit the new data well. 

$V(\hat{f}(x_0))$: Model too complex. Small changes to the data will cause the solution to change a lot. 

Machine learning techniques were developed specifically to maximize prediction performance by providing an empirical way to make this bias-variance trade off

But generally, that means that all our models are somewhat biased (making inference irrelevant)

## Concepts


\alert{Cross validation}: Split data in test and training data. Train model on training data, test it on test data

\alert{Regularization}: A technique used in an attempt to solve overfitting problems

\alert{Supervised Learning}: Models designed to infer a relationship from **labeled** training data.

- linear model selection (OLS, Ridge, Lasso)
- Classification (logistic, KNN, CART)

\alert{Unsupervised Learning}:  Models designed to infer a relationship from **unlabeled** training data.

- PCA 
- KNN 

## Summary

Statistical learning models are designed to optimally trade off bias and variance

This makes them more efficient for prediction than OLS

But also **generally biased** (so they are generally not meant for inference)

Statistical learning models can also be used for **exploratory data analysis**



