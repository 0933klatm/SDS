---
author: Sebastian Barfort
title: "Social Data Science"
subtitle: Causation & Prediction 
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  beamer_presentation:
    keep_tex: no
    theme: metropolis
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    fig_width: 7
    fig_height: 6
    fig_caption: false
    includes:
      in_header: header.tex
fontsize: 10pt
classoption: compress
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
knitr::opts_chunk$set(
              dev= "pdf",
               fig.width=4.25,
               fig.height=2.5,
               fig.show="hold",
               fig.lp="fig:",
               fig.align = "center",
               dpi = 300,
               cache=TRUE,
               par=TRUE,
               echo=TRUE,
               message=FALSE,
               warning=FALSE)
```

## Introduction

What is the objective of empirical policy research? 

1. \alert{causation}: what is the effect of a particular variable on an outcome? 
2. \alert{prediction}: find some function that provides a good prediction of $y$ as a function of $x$

Today: Introduction. 

## Intution

$$ y = \alpha + \beta x + \varepsilon $$

\alert{causation}: $\hat{\beta}$ problem

\alert{prediction}: $\hat{y}$ problem

---

\LARGE Causal Inference

## Introduction

Most econometric theory is focused on estimating **causal effects**

Causal effect: what is the effect of some policy on an outcome we are interested in?

Examples of causal questions:

- what is the effect of immigration on native wages?
- what is the effect of democracy on growth? 
- what is the effect of newspaper coverage on stock prices?

## Intuition

Variable of interest (often called *treatment*): $D_i$

Outcome of interest: $Y_i$

**Potential outcome framework**
$$
Y_i = \left\{
\begin{array}{rl}
Y_{1i} & \text{if } D_i = 1,\\
Y_{0i} & \text{if } D_i = 0
\end{array} \right.
$$

The observed outcome $Y_i$ can be written in terms of potential outcomes as
$$ Y_i = Y_{0i} + (Y_{1i}-Y_{0i})D_i$$

$Y_{1i}-Y_{0i}$ is the *causal* effect of $D_i$ on $Y_i$. 

But we never observe the same individual $i$ in both states. This is the **fundamental problem of causal inference**. 

## Selection Bias I

We need some way of estimating the state we do not observe (the \alert{counterfactual})

Usually, our sample contains individuals from both states

So why not do a naive comparison of averages by treatment status? 
\begin{align}
\nonumber E[Y_i|D_i = 1] - E[Y_i|D_i = 0] = &E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] + \\
 \nonumber  &E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0] 
\end{align}

## Selection Bias II

$E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] = E[Y_{1i} - Y_{0i}|D_i = 1]$: the average *causal* effect of $D_i$ on $Y$. 

$E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]$: difference in average $Y_{0i}$ between the two groups. Likely to be different from 0 when individuals are allowed to self-select into treatment. Often referred to as \alert{selection bias}. 

## Random assignment solves the problem

Random assignment of $D_i$ solves the problem because random assignment makes $D_i$ independent of potential outcomes

That means that $E[Y_{0i}|D_i = 1] = E[Y_{0i}|D_i = 0]$ and thus that the selection bias term is zero

Intuition: with random assignment, non-treated individuals can be used as counterfactuals for treated (*what would have happened to individual $i$ had he not received the treatment*?)

This allows us to overcome the fundamental problem of causal inference

## Randomization

\centering
\emph{no causation without manipulation}

## Who randomizes? 

As mentioned, we need to worry when individuals are allowed to self-select

This means that a lot of thought has to go into the \alert{randomization phase}

Randomization into treatment groups has to be manipulated by someone 

But what about effect of \alert{immutable characteristics} such as race, gender, etc.?

## Quasi Experiments

\alert{Quasi-experiments}: randomization happens by "accident"

- Differences in Differences
- Regression Discontinuity Design
- Instrumental variables

## Randomized Controlled Trials

\alert{Randomized controlled trials}: randomization done by researcher

- Survey experiments
- Field experiments

Note: difficult to say one is strictly better than the other. Randomization can be impractical and/or unethical. 

Can you come up with an example where randomization would be unethical?

## External & internal validity

\alert{Internal validity}: Refers to the validity of causal conclusions

\alert{External validity}: Refers to the extent to which the conclusions of a particular study can be generalized beyond a particular setting

Imai (2016): RCTs trade off external and internal validity

Samii (2016): No such tradeoff. 

## Observational study

In many cases, social scientists are unable to randomize treatment assignment for ethical or logistic reasons

\alert{Observational study}: No random manipulation of treatment

Strategy: Statistical control (control variables, fixed effects, matching, etc)

Risk selection/confounding bias. 

## Pritchett & Sandefur

Pritchett, Lant and Justin Sandefur. 2015. "[Learning from Experiments When Context Matters](https://www.aeaweb.org/articles?id=10.1257/aer.p20151016)." *American Economic Review*, 105(5): 471-75. 

> We analyze the trade-off between internal and external validity faced by a hypothetical policymaker weighing experimental and non- experimental evidence. Empirically, we find that for several prominent questions in develop- ment economics, relying on observational data analysis from within context produces treat- ment effect estimates with lower mean-square error than relying on experimental estimates from another context.


## Racial Discrimination in the Labor Market

Does racial discrimination exist in the labor market?

\alert{Experiment}: In response to newspaper ads, researchers send out resumes of fictitious job candidates, varying only the names of the job applicants while leaving all other information in the resumes unchanges

Names were randomized between stereotypically black- and white-sounding names (Lakisha, Jamal, Emily, Greg, etc.)

## 

```{r}
library("readr")
gh.link = "https://raw.githubusercontent.com/"
user.repo = "kosukeimai/qss/"
branch = "master/"
link = "CAUSALITY/resume.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## 

```{r, echo = FALSE}
knitr::kable(df[1:5, ])
```

## Contingency table

```{r}
library("dplyr")
df.table = df %>% 
  count(race, call)
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Proportions

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo = FALSE}
knitr::kable(df.table)
```

## By gender

```{r}
library("dplyr")
df.table = df %>% 
  group_by(race, sex, call) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

##

```{r, echo = FALSE}
knitr::kable(df.table)
```

## Systematic differences?

Difference in means estimator

```{r}
lin.model = lm(call ~ race == "black", 
               data = df)
```

##

```{r, results = "asis", echo = FALSE}
stargazer::stargazer(lin.model, header = FALSE,
                     no.space = TRUE)
```

## Example: Regression Discontinuity Design

Eggers, Andrew and Alexander Fouirnaies. 2014. "[The Economic Impact of Economic News](http://andy.egge.rs/papers/Eggers_Fouirnaies_Recessions.pdf)."

> We start from the observation that news media pay
considerable attention to a binary distinction between recession and non-recession: by a convention observed in essentially every industrialized country, a recession is announced when an  economy  contracts  for  two  consecutive  quarters. In  cases  where  growth  is  essentially zero, the distinction between a recession and a non-recession becomes highly arbitrary.  Nevertheless (as we confirm below), the media treat fundamentally comparable situations quite differently, producing anxious headlines announcing a recession if growth is barely negative for two consecutive quarters but not if growth is even slightly positive.

## Identification Strategy 

Discontinuous relationship between recession announcements in media and underlying economic fundamentals

Some countries are assigned to *recession* while others are not even when they have the same underlying economic fundamentals (exogenous assignment of treatment)

**Result**: Announcing a recession reduces both consumer confidence and growth in private consumption in the quarter during which the recession is announced

## Results

\centering
![](figures/eggers.png)

## Summary

Causal questions are of key interest to policy makers and academics

The key focus is on \alert{inference}: we want to know about the causal effect of $D$ on $Y$ *in the population of interest*

When you are interested in a **causal question** you need to think carefully about randomization of treatment (this is often referred to as your \alert{identification strategy})

Due to the \alert{fundamental problem of causal inference}, we canâ€™t estimate individual-level causal effects. Instead, we estimate averages.

Is causality the only thing policy makers, firms and social scientists should be interested in?

---

\LARGE Prediction

## Prediction

Many policy problems are not about causality but rather about prediction

Sometimes called *prediction policy problems*

- How many people will sign up for Obamacare?
- Who will win the U.S general election in November?
- Who should the Department of Economics hire in the future?

## Who predicts?

* Local governments -> pension payments/crime/etc
* Google -> whether you will click on an ad
* Netflix -> what movies you will watch
* Insurance companies -> what your risk of death is
* You? -> will *Social Data Science* be a fun/rewarding/interesting course to follow?

## Why predict? Glory!

\centering
![](http://www2.pictures.zimbio.com/gi/Chris+Volinsky+Netflix+Awards+1+Million+Netflix+WOtFSoPeoOal.jpg)

[Netflix Awards $1 Million Prize and Starts a New Contest](http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/?8au&emc=au)

## The Netflix Contest

Competition started in October 2006. Training data is ratings for 18K movies by 400K Netflix customers, each rating between 1 and 5

Training data is very sparse - about 98% missing

Objective is to predict the rating for a set of 1 million customer-movie paris that are missing in the training data

Winner: Averaged 800 models (but the solution never actually implemented)

## Why predict? Riches!

\centering
![](http://kaggle2.blob.core.windows.net/competitions/hhp/2496/logos/header.png)

[http://www.heritagehealthprize.com/c/hhp](http://www.heritagehealthprize.com/c/hhp)

## Example: predicting gender from weight/height

Can we predict gender based on information on an individual's weight/height?

```{r, message = FALSE, warning = FALSE}
gh.link = "https://raw.githubusercontent.com/"
user.repo = "johnmyleswhite/ML_for_Hackers/"
branch = "master/02-Exploration/"
link = "data/01_heights_weights_genders.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## 

```{r, echo = FALSE}
knitr::kable(df[1:5, ])
```

## 

```{r, echo = FALSE}
library("ggplot2")
library("viridis")
ggplot(df, aes(x = Height, fill = Gender)) + 
  geom_density(alpha = .5) +
  scale_fill_viridis(discrete = TRUE)
```
 
## 

```{r, echo = FALSE}
ggplot(sample_frac(df, .5), aes(x = Weight, y = Height)) + 
  geom_point(alpha = .25) + 
  geom_smooth()
```

##

```{r, echo = FALSE}
ggplot(sample_frac(df, .5), aes(x = Weight, y = Height, 
               colour = Gender)) + 
  geom_point(alpha = .1) +
  scale_color_viridis(discrete = TRUE)
```

## Logit model

```{r}
df = df %>% mutate(gender = Gender == "Male")
logit.model = glm(gender ~ Height + Weight, 
                  data = df, 
                  family = binomial(link = "logit"))
```

## The binomial model

Logit estimates

$$ P(Y_i = 1 |X_i = x_i) = \frac{1}{1 + e^{-x_i\beta}}$$

This probability is .5 when $x_i\beta = 0$

So we can classify predicted gender based on height and weight

$$
\hat{y} = \left\{
\begin{array}{rl}
1 & \text{if } x_i\beta \geq 0,\\
0 & \text{otherwise}
\end{array} \right.
$$

## Calculate classification line

**Intercept**: $$ W = \frac{-\alpha - \beta_1 H}{\beta_2} $$

**Slope**: $$ -\frac{\beta_1}{\beta_2} $$

So now we have a classifier: For each combination of weight and height we're able to predict gender based on our logistic model

## 

```{r, echo = FALSE}
buildPoly <- function(xr, yr, slope = 1, intercept = 0, above = TRUE){
    #Assumes ggplot default of expand = c(0.05,0)
    xrTru <- xr + 0.05*diff(xr)*c(-1,1)
    yrTru <- yr + 0.05*diff(yr)*c(-1,1)

    #Find where the line crosses the plot edges
    yCross <- (yrTru - intercept) / slope
    xCross <- (slope * xrTru) + intercept

    #Build polygon by cases
    if (above & (slope >= 0)){
        rs <- data.frame(x=-Inf,y=Inf)
        if (xCross[1] < yrTru[1]){
            rs <- rbind(rs,c(-Inf,-Inf),c(yCross[1],-Inf))
        }
        else{
            rs <- rbind(rs,c(-Inf,xCross[1]))
        }
        if (xCross[2] < yrTru[2]){
            rs <- rbind(rs,c(Inf,xCross[2]),c(Inf,Inf))
        }
        else{
            rs <- rbind(rs,c(yCross[2],Inf))
        }
    }
    if (!above & (slope >= 0)){
        rs <- data.frame(x= Inf,y= -Inf)
        if (xCross[1] > yrTru[1]){
            rs <- rbind(rs,c(-Inf,-Inf),c(-Inf,xCross[1]))
        }
        else{
            rs <- rbind(rs,c(yCross[1],-Inf))
        }
        if (xCross[2] > yrTru[2]){
            rs <- rbind(rs,c(yCross[2],Inf),c(Inf,Inf))
        }
        else{
            rs <- rbind(rs,c(Inf,xCross[2]))
        }
    }
    if (above & (slope < 0)){
        rs <- data.frame(x=Inf,y=Inf)
        if (xCross[1] < yrTru[2]){
            rs <- rbind(rs,c(-Inf,Inf),c(-Inf,xCross[1]))
        }
        else{
            rs <- rbind(rs,c(yCross[2],Inf))
        }
        if (xCross[2] < yrTru[1]){
            rs <- rbind(rs,c(yCross[1],-Inf),c(Inf,-Inf))
        }
        else{
            rs <- rbind(rs,c(Inf,xCross[2]))
        }
    }
    if (!above & (slope < 0)){
        rs <- data.frame(x= -Inf,y= -Inf)
        if (xCross[1] > yrTru[2]){
            rs <- rbind(rs,c(-Inf,Inf),c(yCross[2],Inf))
        }
        else{
            rs <- rbind(rs,c(-Inf,xCross[1]))
        }
        if (xCross[2] > yrTru[1]){
            rs <- rbind(rs,c(Inf,xCross[2]),c(Inf,-Inf))
        }
        else{
            rs <- rbind(rs,c(yCross[1],-Inf))
        }
    }

    return(rs)
}

datPoly1 <- buildPoly(range(df$Height), range(df$Weight),
            slope= - coef(logit.model)[2] / coef(logit.model)[3],
            intercept= - coef(logit.model)[1] /coef(logit.model)[2],
            above = FALSE)
datPoly2 <- buildPoly(range(df$Height), range(df$Weight),
            slope= - coef(logit.model)[2] / coef(logit.model)[3],
            intercept= - coef(logit.model)[1] /coef(logit.model)[2],
            above = TRUE)
ggplot(sample_frac(df, .5), aes(x = Height, y = Weight)) + 
  geom_point(alpha = 0.2) +
  geom_abline(
    intercept = - coef(logit.model)[1] /coef(logit.model)[2],
    slope = - coef(logit.model)[2] / coef(logit.model)[3],
    color = "black") +
  geom_polygon(data=datPoly1, aes(x=x,y=y),alpha=0.2,fill="red") +
  geom_polygon(data=datPoly2, aes(x=x,y=y),alpha=0.2,fill="blue")
```

## 

```{r, echo = FALSE}
df$prediction = logit.model$fitted.values
df$prediction.cat = ifelse(df$prediction >= .5, "Male", "Female")
df$classified = ifelse(df$prediction.cat == df$Gender, "correct", "incorrect")

df.1 = df %>% filter(classified == "correct") %>% sample_frac(.2)
df.2 = df %>% filter(classified != "correct") %>% sample_frac(.75)

ggplot() + 
  geom_point(data = df.1,
             aes(x = Height, y = Weight),
             alpha = .05) +
  geom_point(data = df.2,
             aes(x = Height, y = Weight,
             fill = abs(prediction - .5)), 
             shape = 21, color = "black",
             alpha = .5) +
  geom_abline(
    intercept = - coef(logit.model)[1] / coef(logit.model)[2],
    slope = - coef(logit.model)[2] / coef(logit.model)[3],
    color = "black") + 
  scale_fill_viridis()
```

## Misclassification

How many cases were misclassified?

```{r}
df.class = df %>% 
  mutate(
    pred.prob = predict(logit.model),
    pred.cat = ifelse(pred.prob >= .5, 
                      "Male", "Female"),
    classified = ifelse(prediction.cat == Gender, 
                        "correct", "incorrect")
  ) %>% 
  group_by(classified) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

## 

```{r, echo = FALSE}
knitr::kable(df.class)
```

##

```{r}
df.class = df %>% 
  mutate(
    pred.prob = predict(logit.model),
    pred.cat = ifelse(pred.prob >= .5, 
                      "Male", "Female")
  ) %>% 
  group_by(Gender, pred.cat) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

##

```{r, echo = FALSE}
knitr::kable(df.class)
```

## How well did we do?

Overall, we correctly classified 92% of the cases in our dataset

Is this a good prediction model? 

Is looking at the classification rate on all the data the correct strategy?


## Introduction to statistical learning

Standard empirical techniques are not optimized for prediction problems because they focus on inference

Standard result in econometrics: When there is no omitted variable bias ($E(\varepsilon) = 0$) and the model is homoskedastic ($V(\varepsilon_i) = \sigma^2$) then the OLS estimator is BLUE (Best Linear Unbiased Estimator). 

Keywords: *unbiased* ($E(\hat{\beta}) = \beta$) and *best* (smallest variance among the class of all linear unbiased estimators)

But what about **biased** estimators?

## The bias-variance tradeoff

OLS is designed to minimize *in sample error*: the error rate you get on the same data set you used to build your predictor.

$$ \text{arg min}_{\beta} \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 $$

But for prediction we are interested in minimizing \alert{out of sample error}: the error rate you get on a new data set

## Prediction

Too see this, consider a prediction at a new point, $x_0$. Our prediction for $y_0$ is then $\hat{f}(x_0)$ and the mean squared error (MSE ) can be decomposed as 

$$ E[(y_0 - \hat{f}(x_0))^2] = [\text{Bias}(\hat{f}(x_0))]^2 + V(\hat{f}(x_0)) + \sigma^2$$

By ensuring zero bias, OLS picks a corner solution. This is generally not optimal for prediction. 


## Bias and variance

What do we mean by the *variance* and *bias* of an estimator?

\alert{Bias} ($(\hat{f}(x_0)) = E[\hat{f}(x_0) - f(x_0)]$): Bias refers to the error that is introduced by approximating a real-life problem with a simple model. It won't fit the new data well. 

\alert{Variance} ($V(\hat{f}(x_0))$): Referes to model complexity. If the model is too complex then small changes to the data will cause the solution to change a lot. 

Machine learning techniques were developed specifically to maximize prediction performance by providing an empirical way to make this bias-variance trade off

But generally, that means that all our models are somewhat biased (making inference irrelevant)

## Concepts


\alert{Cross validation}: Split data in test and training data. Train model on training data, test it on test data

\alert{Regularization}: A technique used in an attempt to solve overfitting problems

\alert{Supervised Learning}: Models designed to infer a relationship from **labeled** training data.

- linear model selection (OLS, Ridge, Lasso)
- Classification (logistic, KNN, CART)

\alert{Unsupervised Learning}:  Models designed to infer a relationship from **unlabeled** training data.

- PCA 
- KNN 

## Summary

Statistical learning models are designed to optimally trade off bias and variance

This makes them more efficient for prediction than OLS

But also **generally biased** (so they are generally not meant for inference)

Statistical learning models can also be used for **exploratory data analysis**

---

\LARGE Models in `R`

## Linear Regression

Linear regression is a simple approach to supervised learning. Assumes that the dependence of $Y$ on $X_1, ..., X_n$ is linear

Assumes a model of

$$ Y = \alpha + \beta X + \varepsilon$$

Where $\alpha$ and $\beta$ are unknown constants to be estimated from the data. 

When we've obtained these estimates, we can predict values of the dependent variable by plugging in new values of $X$

$$ \hat{y} = \hat{\alpha} + \hat{\beta}X $$

## R

```{r}
library("readr")
gh.link = "https://raw.githubusercontent.com/"
user.repo = "sebastianbarfort/sds_summer/"
branch = "gh-pages/"
link = "data/bball.csv"
data.link = paste0(gh.link, user.repo, branch, link)
df = read_csv(data.link)
```

## Predict points

```{r}
model.1 = lm(
  pts ~ height + weight + fg.pct + ft.pct, 
  data = df)
```

## Useful functions and packages

- `stargazer`: convert model to LaTeX
- `broom`: convert model to tidy format
- `summary`: summarise model output
- `predict`: predict new $y$ based on $x$

## 

```{r, eval = FALSE}
library("stargazer")
stargazer(model.1)
```

## 

```{r, echo = FALSE, results = "asis"}
library("stargazer")
stargazer(model.1, header = FALSE, style = "ajps", no.space = TRUE,
          keep.stat = "n")
```

## 

```{r}
library("broom")
output.1 = model.1 %>% tidy
```

## 

```{r, echo = FALSE}
knitr::kable(output.1)
```

## 

```{r, echo = FALSE}
output.1 = output.1 %>% 
  filter(term != "(Intercept)") 
```

```{r}
p = ggplot(output.1, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2, 
             colour = "white") +
  geom_point() +
  geom_errorbar(aes(ymin=estimate-2*std.error, 
                    ymax=estimate+2*std.error)) +
  coord_flip()
```

## Coefficient Plot

```{r, echo = FALSE}
p 
```

## Logistic Regression

```{r}
df = df %>% mutate(pts.high = pts > 13)
model.2 = glm(
  pts.high ~ height + weight + fg.pct + ft.pct, 
  data = df,
  family = binomial(link = "logit"))
```  
  
## 

```{r}
library("modelr")
df.plot = df %>% 
  data_grid(fg.pct = seq_range(fg.pct, 50), 
            .model = model.2) 
preds = predict(model.2,
                newdata = df.plot, 
                type = "response",
                se = TRUE)
df.plot$pred.full = preds$fit
df.plot$ymin = df.plot$pred.full - 2*preds$se.fit
df.plot$ymax = df.plot$pred.full + 2*preds$se.fit  
```  
  
## 

```{r, echo = FALSE}
knitr::kable(df.plot[1:6, 1:6])
```
  
##

```{r}
p = ggplot(df.plot, aes(x = fg.pct, y = pred.full)) + 
    geom_ribbon(aes(y = pred.full, 
                    ymin = ymin, 
                    ymax = ymax),alpha = 0.25) +
    geom_line(color = "blue")
```  

## 

```{r, echo = FALSE}
p +
  labs(x = "Field Goal Percentage",
       y = "Predicted Probability")
```


