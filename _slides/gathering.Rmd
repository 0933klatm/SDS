---
author: Sebastian Barfort
title: "Social Data Science"
subtitle: Data Gathering
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  beamer_presentation:
    keep_tex: no
    theme: metropolis
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    fig_width: 7
    fig_height: 6
    fig_caption: false
    includes:
      in_header: header.tex
fontsize: 10pt
classoption: compress
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
knitr::opts_chunk$set(
              dev= "pdf",
               fig.width=4.25,
               fig.height=2.5,
               fig.show="hold",
               fig.lp="fig:",
               fig.align = "center",
               dpi = 300,
               cache=TRUE,
               par=TRUE,
               echo=TRUE,
               message=FALSE,
               warning=FALSE)
```

## Ethics

### On the ethics of web scraping and data journalism

> If an institution publishes data on its website, this data should automatically be public

> If a regular user can't access the data, we shouldn't try to get it (that would be hacking)

> Always read the user terms and conditions

> Always check the `robots.txt` file, which states what is allowed to be scraped


## Rules of web scraping

1. You should check a site's terms and conditions before you scrape them. It's their data and they likely have some rules to govern it.
2. Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don't hammer the site's server.
3. Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.
4. Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data.

##

\centering
![](figures/hackingFT.png)

## How does a web page look like? 

\centering
https://sebastianbarfort.github.io/

## Motivating example 

\centering 
https://en.wikipedia.org/wiki/Table_%28information%29

## Example

`rvest` is a nice R package for scraping web pages that don't have an API

To extract something, you start with [selectorgadget](http://selectorgadget.com/) to figure out which `css` selector matches the data we want

Selectorgadget is a browser extension for quickly extracting desired parts of an HTML page.

With some user feedback, the gadget find out the CSS selector that returns the highlighted page elements.

## 
```{r}
library("rvest")
link = paste0("http://en.wikipedia.org/",
              "wiki/Table_(information)")
link.data = link %>%
  read_html() %>% 
  html_node(".wikitable") %>% 
  # extract first node with class wikitable
  html_table() 
  # then convert the HTML table into a data frame
```

`html_table` usually only works on 'nicely' formatted HTML tables.

## 

```{r, echo = FALSE}
knitr::kable(link.data)
```

## 

This is a nice format? Really? Yes, really. It's the format used to render tables on webpages (remember: [programming sucks](https://www.stilldrinking.org/programming-sucks))

```html
<table class="wikitable">
  <tr>
    <th>First name</th>
    <th>Last name</th>
    <th>Age</th>
  </tr>
  <tr>
    <td>Bielat</td>
    <td>Adamczak</td>
    <td>24</td>
  </tr>
   ...
</table> 
```

## Scraping Jyllands Posten

\centering
http://jyllands-posten.dk/

## Scraping Jyllands Posten in `rvest`

Assume we want to extract the headlines

- Fire up Selectorgadget
- Find the correct selector
    - `css selector`: `.artTitle a`
    - Want to use xpath? no problem.

## Scraping headlines

```{r}
css.selector = ".artTitle a"
link = "http://jyllands-posten.dk/"

jp.data = link %>% 
  read_html() %>% 
  html_nodes(css = css.selector) %>% 
  html_text()
```

## 

```{r, echo = FALSE}
jp.data[1:5]
```


## Garbage

Notice that there are still some garbage characters in the scraped text

So we need our string processing skills to clean the scraped data 

Can be done in many ways

```{r}
library("stringr")
jp.data1 = jp.data %>% 
  str_replace_all(pattern = "\\n|\\t|\\r" , 
                  replacement = "") 
```

## 

```{r, echo = FALSE}
knitr::kable(jp.data1[1:6])
```

## `str_trim`

`str_trim`: Trim whitespace from start and end of string

```{r}
library("stringr")
jp.data2 = jp.data %>% 
  str_trim()
```

## 

```{r, echo = FALSE}
knitr::kable(jp.data2[1:6])
```

## Extracting attributes

What if we also wanted the links embedded in those headlines?

```{r}
jp.links = link %>% 
  read_html(encoding = "UTF-8") %>% 
  html_nodes(css = css.selector) %>%
  html_attr(name = 'href')
```

## 

```{r, echo = FALSE}
knitr::kable(jp.links[1:6])
```

## Looping through collection of links 

We now have `jp.links`, a vector of all the links to news stories from JP's front page

Let's loop through every link and extract some information.

## Cleaning the vector

Assume that we're only interested in domestic and international politics 

```{r}
jp.keep = jp.links %>%
  str_detect("politik|indland|international")
jp.links.clean = jp.links[jp.keep]
jp.remove.index = jp.links.clean %>% 
  str_detect("protected|premium|finans")
jp.links.clean = jp.links.clean[!jp.remove.index]
```

## Grab info from first link

```{r}
first.link = jp.links.clean[1]
first.link.text = first.link %>% 
  read_html(encoding = "UTF-8") %>% 
  html_nodes("#articleText") %>% 
  html_text()
```

## 

```{r, echo = FALSE}
first.link.text
```

## While we're at it...

Let's also grab the author of the article

```{r}
first.link %>% 
  read_html(encoding = "UTF-8") %>% 
  html_nodes(".bylineAuthorName span") %>% 
  html_text()
```

## Turning it into a function

\alert{Function}: automate the boring stuff. 

\alert{Iteration}: apply a function to many elements. 

Let's write a function that for each new link will return article text.

## Scraping function

```{r}
scrape_jp = function(link){
  my.link = link %>% 
    read_html(encoding = "UTF-8")
  author = my.link %>% 
    html_nodes(".bylineAuthorName span") %>% 
    html_text()
  if (length(author) == 0){ author = NA }
  link.text = my.link %>% 
    html_nodes("#articleText") %>% 
    html_text() 
  if (length(link.text) == 0){ link.text = NA }
  return(data.frame(author = author,
    link = link, text = link.text ))
}
```

## 

Now we can iterate through all the links and grab the data

```{r}
library("purrr")
jp.article.data = jp.links.clean[1:5] %>% 
  map_df(scrape_jp)
```

##

### Output

```{r, echo  = FALSE}
knitr::kable(dplyr::glimpse(jp.article.data))
```

## Exercise 

1. Go to http://www.econ.ku.dk/ansatte/vip/
2. Create a vector of all links to the researcher's personal home page
3. Go to each researchers page and grab their title
4. Create a data frame of all researchers' names and title

##

```{r, echo = FALSE}
econ.link = "http://www.econ.ku.dk/ansatte/vip/"
links = econ.link %>% 
  read_html(encoding = "UTF-8") %>%
  html_nodes("td:nth-child(1) a")%>%
  html_attr(name = 'href')

long.links = paste(econ.link, links, sep = "")

scrape_econ = function(link){
  my.link = link %>% 
    read_html(encoding = "UTF-8")
  title = my.link %>% 
    html_nodes("#content .type") %>% 
    html_text()
  title = title[1]
  name = my.link %>% 
    html_nodes(".person") %>% 
    html_text()
  name = name[1]
  return( data.frame(name = name, 
                     title = title))
}

econ.data = long.links[1:5] %>% 
  map_df(scrape_econ)

knitr::kable(econ.data[1:5, ])
```

---

\LARGE Gathering data from APIs

## API

API: Application Program Interface

Programmatic instructions for how to interact 
with a piece of software

Many data sources have API's - largely for talking to other web interfaces

Consists of a set of methods to search, retrieve, or submit data to, a data source

We can write R code to interface with an API (lot's require authentication though)

Many packages already connect to well-known API's 

## APIs

Most APIs are `REST` APIs

Implemented in `R` in `httr` package.

**GET**: \alert{Retrieve} whatever is specified by the URL

**POST**: \alert{Create} resource at URL with given data


## Example: Github Issues

\centering
https://developer.github.com/v3/issues/

##

```{r}
library("httr")
url = "https://api.github.com/repos/hadley/dplyr/issues"
get.1 = GET(url, query = list(state = "closed"))
get.2 = GET(url, query = list(state = "closed",
                       labels = "bug"))
```

## API output

Output from APIs come in one of two formats: `XML` or `JSON`

**JSON**: Javascript Object Notation

- Widely used in web APIs
- Becoming de facto standard for online data format
- Read into `R` with `jsonlite` package

**XML**: Extensible Markup Language

- Less common today
- Read into `R` with `xml2` package

## 

### JSON

```{json}
{
"Title": "Frozen",
"Year": "2013",
"Rated": "PG",
"Released": "27 Nov 2013",
"Runtime": "102 min",
"Genre": "Animation, Adventure, Comedy",
"Director": "Chris Buck, Jennifer Lee"
...
}
```

##

### XML

```{xml}
<?xml version="1.0"?>
<catalog>
   <book id="bk101">
      <author>Gambardella, Matthew</author>
      <title>XML Developer's Guide</title>
      <genre>Computer</genre>
      <price>44.95</price>
      <publish_date>2000-10-01</publish_date>
      <description>An in-depth look at creating applications 
      with XML.</description>
   </book>
```

## Parsing

```{r}
library("jsonlite")
get.1.parsed = content(get.1, as = "text")
get.1.data = fromJSON(get.1.parsed, flatten = TRUE)
get.2.parsed = content(get.2, as = "text")
get.2.data = fromJSON(get.2.parsed, flatten = TRUE)
```

##

### `get.1.data`

```{r, echo = FALSE}
knitr::kable(get.1.data[1:8, c("number",
                               "comments", 
                               "user.login", "closed_at")])
```

##

### `get.2.data`

```{r, echo = FALSE}
knitr::kable(get.2.data[1:8, c("number",
                               "comments", 
                               "user.login", "closed_at")])
```

## Packages for working with APIs

Luckily, you rarely have to access APIs manually

`R` already has *a lot of* packages for easy access to many APIs

Check some of them out [here](https://github.com/ropensci/webservices)

## `twitteR`

`twitteR` is an R package which provides access to the Twitter API

Create an app [here](https://apps.twitter.com/) 

```{r, eval = FALSE}
library("twitteR")
consumer_key = 'your key'
consumer_secret = 'your secret'
access_token = 'your access token'
access_secret = 'your access secret'

setup_twitter_oauth(consumer_key,
                    consumer_secret,
                    access_token,
                    access_secret)

searchTwitter("#dkpol", n=500)
```

## Interesting API packages

\centering
[rfacebook](https://github.com/pablobarbera/Rfacebook)

[streamR](https://github.com/pablobarbera/streamR)

[instaR](https://github.com/pablobarbera/instaR)

[rtimes](https://github.com/rOpenGov/rtimes)

[tuber](https://github.com/soodoku/tuber)

[ggmap](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)

## `gmapsdistance`

Another useful package is `gmapsdistance`

It uses the [Google Maps Distance Matrix API](https://developers.google.com/maps/documentation/distance-matrix/intro?hl=en) to compute the distance(s) and time(s) between two points or two vectors of points

```{r, eval = FALSE}
install.packages("gmapsdistance")
```

##

```{r}
library("gmapsdistance")
results = gmapsdistance(origin = "København", 
                        destination = "Roskilde", 
                        mode = "driving")
```

## 

```{r, echo = FALSE}
results
```

##

Compute walking distance between Marathon and Athens

```{r}
results = gmapsdistance(
  origin = "38.1621328+24.0029257",
  destination = "37.9908372+23.7383394",
  mode = "walking")
```

## 

```{r, echo = FALSE}
results
```

## Geocode

```{r}
library("ggmap")
geocode("Økonomisk Institut, 
        Københavns Universitet, København")
```

##

```{r}
geocode("The White House")
```

## Statistics Denmark API

https://github.com/rOpenGov/dkstat

Lets you programatically work with Statistics Denmark data

```{r, eval = FALSE}
library("devtools")
install_github("rOpenGov/dkstat")
```

## Extensions

[selanium](https://cran.rstudio.com/web/packages/RSelenium/)

[selaniumPipes](https://github.com/johndharrison/seleniumPipes)

[tabulizer](https://github.com/leeper/tabulizer)


